# MetaVoice-1B

ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржирж╛ржХрж┐ ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржХрж┐ржиржмрзЗржи?

ржжрзЗрж╢рзЗ рждрж╛ржкржорж╛рждрзНрж░рж╛ ржкрзНрж░ржЪржгрзНржб ржмрзЗрзЬрзЗ ржЧрзЗржЫрзЗред рждрзАржмрзНрж░ ржЧрж░ржорзЗ ржПржХржЯрзБ ржкрзНрж░рж╢рж╛ржирзНрждрж┐рж░ ржЬржирзНржп ржЕржирзЗржХрзЗржЗ ржПрж╕рж┐ ржХрж┐ржирждрзЗ ржЪрж╛ржЪрзНржЫрзЗржи ржПржЦржиред ржПржХ рж╕ржорзЯржХрж╛рж░ ржЙржЪрзНржЪржмрж┐рждрзНрждржжрзЗрж░ ржмрж┐рж▓рж╛рж╕рж┐рждрж╛ ржПрж╕рж┐ ржПржЦржи ржЙржЪрзНржЪржмрж┐рждрзНржд-ржоржзрзНржпржмрж┐рждрзНрждрзЗрж░ ржкрзНрж░рзЯрзЛржЬржирзАрзЯрждрж╛рзЯ рж░рзВржк ржирж┐рзЯрзЗржЫрзЗред ржпрж╛рж░рж╛ ржирждрзБржи ржПрж╕рж┐ ржХрзЗржирж╛рж░ ржХржерж╛ ржнрж╛ржмржЫрзЗржи, рждрж╛ржжрзЗрж░ ржХрж░рждрзЗ рж╣ржЪрзНржЫрзЗ ржирж╛ржирж╛ ржЪрж┐ржирзНрждрж╛ржнрж╛ржмржирж╛ред ржХрзЛржи ржПрж╕рж┐ ржХрж┐ржирж▓рзЗ ржмрж┐ржжрзНржпрзБрждрзЗрж░ ржмрж┐рж▓ ржХржо ржЖрж╕ржмрзЗред ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржирж╛ржХрж┐ ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржХрж┐ржиржмрзЗржи?


ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржХрзА?

ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржмрзЬ рж╕рзБржмрж┐ржзрж╛ рж╣ржЪрзНржЫрзЗ, ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржХржоржкрзНрж░рзЗрж╕рж░ ржорзЛржЯрж░ржЯрж┐ ржкрзНрж░рзЯрзЛржЬржиржорждрзЛ рждрж╛рж░ ржЪрж▓рж╛рж░ ржЧрждрж┐ ржкрж░рж┐ржмрж░рзНрждржи ржХрж░рждрзЗ ржкрж╛рж░рзЗред ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рждрзЗ ржПржоржи ржПржХржЯрж┐ рж╕рзЗржирзНрж╕рж░ ржерж╛ржХрзЗ, ржпрж╛ ржШрж░рзЗрж░ рждрж╛ржкржорж╛рждрзНрж░рж╛рж░ ржУржкрж░ ржирж┐рж░рзНржнрж░ ржХрж░рзЗ, ржХржоржкрзНрж░рзЗрж╕рж░ ржкрзБрж░рзЛржкрзБрж░рж┐ ржмржирзНржз ржирж╛ ржХрж░рзЗ, ржорзЛржЯрж░ржЯрж┐рж░ ржЪрж▓рж╛рж░ ржЧрждрж┐ ржХржорж┐рзЯрзЗ ржжрзЗрзЯред ржПрж░ ржХрж╛рж░ржгрзЗржЗ ржмрж┐ржжрзНржпрзБрзО ржЦрж░ржЪ ржХржорзЗ ржЖрж╕рзЗред ржХрж┐ржирзНрждрзБ ржЕржирзНржпржжрж┐ржХрзЗ ржиржи ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржХржоржкрзНрж░рзЗрж╕рж░ ржмрж╛рж░ржмрж╛рж░ ржЪрж╛рж▓рзБ-ржмржирзНржз рж╣рзЯ, рждрж╛ржЗ ржЕржирзЗржХ ржмрзЗрж╢рж┐ ржмрж┐ржжрзНржпрзБрзО ржЦрж░ржЪ рж╣рзЯред ржПржЯрж┐ рж╕рзБржирж┐рж░рзНржжрж┐рж╖рзНржЯ рждрж╛ржкржорж╛рждрзНрж░рж╛ ржирж┐ржпрж╝ржирзНрждрзНрж░ржг, ржнрж╛рж▓рзЛ ржжржХрзНрж╖рждрж╛ ржПржмржВ ржХржо рж╢ржмрзНржжрзЗрж░ ржорж╛рждрзНрж░рж╛ ржмржЬрж╛ржпрж╝ рж░рж╛ржЦрждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗред


ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржХрзА?

ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржорзВрж▓ржд ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржмрж┐ржкрж░рзАрждред ржПржЧрзБрж▓рж┐ ржлрж┐ржХрзНрж╕ржб-рж╕рзНржкрж┐ржб ржХржорзНржкрзНрж░рзЗрж╕рж╛рж░рж╕рж╣ ржкрзНрж░ржЪрж▓рж┐ржд ржПрж╕рж┐ред ржПржХржЯрж┐ ржиржи ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рждрзЗ ржПрж░ ржХржорзНржкрзНрж░рзЗрж╕рж╛рж░ржЯрж┐ ржПржХржЯрж┐ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЧрждрж┐рждрзЗ ржЪрж▓рждрзЗ ржерж╛ржХрзЗ, ржПрж░ ржорж╛ржирзЗ ржХржорзНржкрзНрж░рзЗрж╕рж╛рж░ржЯрж┐ ржЪрж╛рж▓рзБ рж░рзЗржЦрзЗ ржжрж┐рж▓рзЗ ржШрж░рзЗрж░ ржнрзЗрждрж░рзЗрж░ рждрж╛ржкржорж╛рждрзНрж░рж╛ ржХрзНрж░ржорж╛ржЧржд ржХржорждрзЗржЗ ржерж╛ржХржмрзЗред ржШрж░рзЗрж░ ржарж╛ржирзНржбрж╛ ржХрж┐ржЫрзБржЯрж╛ ржмрзЗрж╢рж┐ рж╣рзЯрзЗ ржЧрзЗрж▓рзЗржЗ, ржПрж░ ржХржорзНржкрзНрж░рзЗрж╕рж╛рж░ржЯрж┐ ржмржирзНржз ржХрж░рзЗ ржжрзЗрзЯ, ржЖржмрж╛рж░ ржЧрж░ржо ржХрж┐ржЫрзБржЯрж╛ ржмрзЗрзЬрзЗ ржЧрзЗрж▓рзЗ ржЖржмрж╛рж░ рж╕рзЗржЯрж┐ржХрзЗ ржЪрж╛рж▓рзБ ржХрж░рзЗ ржжрзЗрзЯред ржПржЗ ржЕржи ржЕржл рж╣ржУрзЯрж╛рж░ ржмрзНржпрж╛ржкрж╛рж░ржЯрж╛ ржкрзБрж░рзЛржкрзБрж░рж┐ ржПржХржЬржи ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ рж╕рзЗржЯрж┐ржВрж╕ ржмрж╛ ржирж┐рж░рзНржзрж╛рж░рж┐ржд рждрж╛ржкржорж╛рждрзНрж░рж╛рж░ ржЙржкрж░ ржирж┐рж░рзНржнрж░ ржХрж░рзЗред

ржХрзЛржи ржПрж╕рж┐ ржнрж╛рж▓рзЛ?

ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржХржорзНржкрзНрж░рзЗрж╕рж╛рж░ ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржоржд ржмрж╛рж░ ржмрж╛рж░ ржЪрж╛рж▓рзБ ржПржмржВ ржмржирзНржз рж╣рзЯ ржирж╛ред ржкрзНрж░рзЯрзЛржЬржиржоржд ржЧрждрж┐ ржХржорж┐рзЯрзЗ ржжрзАрж░рзНржШ рж╕ржорзЯ ржзрж░рзЗ ржЪрж▓рждрзЗ ржерж╛ржХрзЗред ржЕржирзНржпржжрж┐ржХрзЗ, ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рж░ ржХржорзНржкрзНрж░рзЗрж╕рж╛рж░ ржмрж╛рж░ ржмрж╛рж░ ржмрзЗрж╢ рж╢ржмрзНржж ржХрж░рзЗ ржЪрж╛рж▓рзБ ржПржмржВ ржмржирзНржз рж╣рзЯред ржПржоржиржХрж┐ ржпржЦржи ржЪрж▓рзЗ рждржЦржиржУ ржЕржзрж┐ржХрж╛ржВрж╢ ржХрзНрж╖рзЗрждрзНрж░рзЗржЗ ржмрзЗрж╢ рж╢ржмрзНржж рждрзИрж░рж┐ ржХрж░рзЗред рждрж╛ржЗ ржП ржХрзНрж╖рзЗрждрзНрж░рзЗржУ ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ рж╕рж╛ржзрж╛рж░ржг ржПрж╕рж┐ ржерзЗржХрзЗ ржПржЧрж┐рзЯрзЗ ржерж╛ржХржЫрзЗред

ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐рждрзЗ ржЖржкржирж╛рж░ ржШрж░ ржжрзНрж░рзБржд ржарж╛ржирзНржбрж╛ рж╣ржмрзЗ, ржХрж╛рж░ржг ржПрж╕рж┐ ржЪрж╛рж▓рзБ рж╣ржмрж╛рж░ рж╕ржорзЯрзЗ ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ рж╕рж╛ржзрж╛рж░ржг ржПрж╕рж┐рж░ ржЪрзЗрзЯрзЗ ржмрзЗрж╢рж┐ рж╢ржХрзНрждрж┐ ржЦрж░ржЪ ржХрж░рзЗ ржжрзНрж░рзБржд ржШрж░ ржарж╛ржирзНржбрж╛ ржХрж░рзЗ, ржПрж░ржкрж░ рж╢ржХрзНрждрж┐ ржХржорж┐рзЯрзЗ ржзрзАрж░рзЗ ржЪрж▓рждрзЗ ржерж╛ржХрзЗред ржЕржирзНржпржжрж┐ржХрзЗ, ржиржи-ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржЪрж╛рж▓рзБрж░ рж╕ржорзЯ рж╕рзНржмрж╛ржнрж╛ржмрж┐ржХ рж╢ржХрзНрждрж┐рждрзЗржЗ ржЪрж╛рж▓рзБ рж╣рзЯ, ржХрж┐ржЫрзБ рж╕ржорзЯ ржзрж░рзЗ ржШрж░ ржарж╛ржирзНржбрж╛ ржХрж░рж╛рж░ ржкрж░ ржЖржмрж╛рж░ ржкрзБрж░рзЛ ржмржирзНржз рж╣рзЯред ржШрж░рзЗрж░ рждрж╛ржкржорж╛рждрзНрж░рж╛ ржмрзЗрзЬрзЗ ржЧрзЗрж▓рзЗ ржЖржмрж╛рж░ ржЪрж╛рж▓рзБ рж╣рзЯред                                       ржнрж┐рж╢ржи ржжрж┐ржЪрзНржЫрзЗ рж╕ржоржпрж╝рзЗрж░ рж╕рзЗрж░рж╛ ржмрж┐ржжрзНржпрзБрзО ржмрж┐рж▓ рж╕рж╛рж╢рзНрж░ржпрж╝рзА ржбрзБржпрж╝рзЗрж▓ ржЗржиржнрж╛рж░рзНржЯрж╛рж░ ржПрж╕рж┐ ржпрж╛ ржЖржкржирж╛рж░ ржШрж░ржХрзЗ рж░рж╛ржЦржм ржарж╛ржирзНржбрж╛- рж╕рж╛ржерзЗ ржкрж╛ржЪрзНржЫрзЗржи ржлрзНрж░рж┐ ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржПржмржВ ржпрж╛ржмрждрзАржпрж╝ рж╕рж░ржЮрзНржЬрж╛ржо ржлрзНрж░рж┐!!!!! ржпрзЛржЧрж╛ржпрзЛржЧ ржнрж┐рж╢ржи ржЗржорзНржкрзЛрж░рж┐ржпрж╝рж╛ржо- ржирж╛ржЩрзНржЧрж▓ржХрзЛржЯ,ржЬрж╛ржорж╛ржирж╕ ржкрзНрж▓рж╛ржЬрж╛,ржорзМржХрж╛рж░рж╛ рж╕рж┐ ржПржи ржЬрж┐ рж╕рзНржЯтАНрзНржпрж╛ржирзНржб, ржирж╛ржЩрзНржЧрж▓ржХрзЛржЯ -                        ржорзЛржмрж╛ржЗрж▓: 01841557067

[![Playground](https://img.shields.io/static/v1?label=Try&message=Playground&color=fc4982&url=https://ttsdemo.themetavoice.xyz/)](https://ttsdemo.themetavoice.xyz/)
<a target="_blank" style="display: inline-block; vertical-align: middle" href="https://colab.research.google.com/github/metavoiceio/metavoice-src/blob/main/colab_demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
[![](https://dcbadge.vercel.app/api/server/Cpy6U3na8Z?style=flat&compact=True)](https://discord.gg/tbTbkGEgJM)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/OnusFM.svg?style=social&label=@metavoiceio)](https://twitter.com/metavoiceio)



MetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities:
* **Emotional speech rhythm and tone** in English.
* **Zero-shot cloning for American & British voices**, with 30s reference audio.
* Support for (cross-lingual) **voice cloning with finetuning**.
  * We have had success with as little as 1 minute training data for Indian speakers.
* Synthesis of **arbitrary length text**

WeтАЩre releasing MetaVoice-1B under the Apache 2.0 license, *it can be used without restrictions*.


## Quickstart - tl;dr

Web UI
```bash
docker-compose up -d ui && docker-compose ps && docker-compose logs -f
```

Server
```bash
docker-compose up -d server && docker-compose ps && docker-compose logs -f
```

## Installation

**Pre-requisites:**
- GPU VRAM >=12GB
- Python >=3.10,<3.12
- pipx ([installation instructions](https://pipx.pypa.io/stable/installation/))

**Environment setup**
```bash
# install ffmpeg
wget https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz
wget https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz.md5
md5sum -c ffmpeg-git-amd64-static.tar.xz.md5
tar xvf ffmpeg-git-amd64-static.tar.xz
sudo mv ffmpeg-git-*-static/ffprobe ffmpeg-git-*-static/ffmpeg /usr/local/bin/
rm -rf ffmpeg-git-*

# install rust if not installed (ensure you've restarted your terminal after installation)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

### Project dependencies installation
1. [Using poetry](#using-poetry-recommended)
2. [Using pip/conda](#using-pipconda)

#### Using poetry (recommended)
```bash
# install poetry if not installed (ensure you've restarted your terminal after installation)
pipx install poetry

# disable any conda envs that might interfere with poetry's venv
conda deactivate

# if running from Linux, keyring backend can hang on `poetry install`. This prevents that.
export PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring

# pip's dependency resolver will complain, this is temporary expected behaviour
# full inference & finetuning functionality will still be available
poetry install && poetry run pip install torch==2.2.1 torchaudio==2.2.1
```

#### Using pip/conda
NOTE 1: When raising issues, we'll ask you to try with poetry first.
NOTE 2: All commands in this README use `poetry` by default, so you can just remove any `poetry run`.

```bash
pip install -r requirements.txt
pip install torch==2.2.1 torchaudio==2.2.1
pip install -e .
```

## Usage
1. Download it and use it anywhere (including locally) with our [reference implementation](/fam/llm/fast_inference.py)
```bash
# You can use `--quantisation_mode int4` or `--quantisation_mode int8` for experimental faster inference.  This will degrade the quality of the audio.
# Note: int8 is slower than bf16/fp16 for undebugged reasons. If you want fast, try int4 which is roughly 2x faster than bf16/fp16.
poetry run python -i fam/llm/fast_inference.py

# Run e.g. of API usage within the interactive python session
tts.synthesise(text="This is a demo of text to speech by MetaVoice-1B, an open-source foundational audio model.", spk_ref_path="assets/bria.mp3")
```
> Note: The script takes 30-90s to startup (depending on hardware). This is because we torch.compile the model for fast inference.

> On Ampere, Ada-Lovelace, and Hopper architecture GPUs, once compiled, the synthesise() API runs faster than real-time, with a Real-Time Factor (RTF) < 1.0.

2. Deploy it on any cloud (AWS/GCP/Azure), using our [inference server](serving.py) or [web UI](app.py)
```bash
# You can use `--quantisation_mode int4` or `--quantisation_mode int8` for experimental faster inference. This will degrade the quality of the audio.
# Note: int8 is slower than bf16/fp16 for undebugged reasons. If you want fast, try int4 which is roughly 2x faster than bf16/fp16.
poetry run python serving.py
poetry run python app.py
```

3. Use it via [Hugging Face](https://huggingface.co/metavoiceio)
4. [Google Colab Demo](https://colab.research.google.com/github/metavoiceio/metavoice-src/blob/main/colab_demo.ipynb)

## Finetuning
We support finetuning the first stage LLM (see [Architecture section](#Architecture)).

In order to finetune, we expect a "|"-delimited CSV dataset of the following format:

```csv
audio_files|captions
./data/audio.wav|./data/caption.txt
```

Note that we don't perform any dataset overlap checks, so ensure that your train and val datasets are disjoint.

Try it out using our sample datasets via:
```bash
poetry run finetune --train ./datasets/sample_dataset.csv --val ./datasets/sample_val_dataset.csv
```

Once you've trained your model, you can use it for inference via:
```bash
poetry run python -i fam/llm/fast_inference.py --first_stage_path ./my-finetuned_model.pt
```

### Configuration

In order to set hyperparameters such as learning rate, what to freeze, etc, you
can edit the [finetune_params.py](./fam/llm/config/finetune_params.py) file.

We've got a light & optional integration with W&B that can be enabled via setting
`wandb_log = True` & by installing the appropriate dependencies.

```bash
poetry install -E observable
```

## Upcoming
- [x] Faster inference тЪб
- [x] Fine-tuning code ЁЯУР
- [ ] Synthesis of arbitrary length text


## Architecture
We predict EnCodec tokens from text, and speaker information. This is then diffused up to the waveform level, with post-processing applied to clean up the audio.

* We use a causal GPT to predict the first two hierarchies of EnCodec tokens. Text and audio are part of the LLM context. Speaker information is passed via conditioning at the token embedding layer. This speaker conditioning is obtained from a separately trained speaker verification network.
  - The two hierarchies are predicted in a "flattened interleaved" manner, we predict the first token of the first hierarchy, then the first token of the second hierarchy, then the second token of the first hierarchy, and so on.
  - We use condition-free sampling to boost the cloning capability of the model.
  - The text is tokenised using a custom trained BPE tokeniser with 512 tokens.
  - Note that we've skipped predicting semantic tokens as done in other works, as we found that this isn't strictly necessary.
* We use a non-causal (encoder-style) transformer to predict the rest of the 6 hierarchies from the first two hierarchies. This is a super small model (~10Mn parameters), and has extensive zero-shot generalisation to most speakers we've tried. Since it's non-causal, we're also able to predict all the timesteps in parallel.
* We use multi-band diffusion to generate waveforms from the EnCodec tokens. We noticed that the speech is clearer than using the original RVQ decoder or VOCOS. However, the diffusion at waveform level leaves some background artifacts which are quite unpleasant to the ear. We clean this up in the next step.
* We use DeepFilterNet to clear up the artifacts introduced by the multi-band diffusion.

## Optimizations
The model supports:
1. KV-caching via Flash Decoding
2. Batching (including texts of different lengths)

## Contribute
- See all [active issues](https://github.com/metavoiceio/metavoice-src/issues)!

## Acknowledgements
We are grateful to Together.ai for their 24/7 help in marshalling our cluster. We thank the teams of AWS, GCP & Hugging Face for support with their cloud platforms.

- [A D├йfossez et. al.](https://arxiv.org/abs/2210.13438) for Encodec.
- [RS Roman et. al.](https://arxiv.org/abs/2308.02560) for Multiband Diffusion.
- [@liusongxiang](https://github.com/liusongxiang/ppg-vc/blob/main/speaker_encoder/inference.py) for speaker encoder implementation.
- [@karpathy](https://github.com/karpathy/nanoGPT) for NanoGPT which our inference implementation is based on.
- [@Rikorose](https://github.com/Rikorose) for DeepFilterNet.

Apologies in advance if we've missed anyone out. Please let us know if we have.
